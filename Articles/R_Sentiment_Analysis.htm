MENU<#>

  * Home <https://www.r-bloggers.com/>
  * About <https://www.r-bloggers.com/about/>
  * RSS <https://feeds.feedburner.com/RBloggers>
  * add your blog! <https://www.r-bloggers.com/add-your-blog/>
  * Learn R <https://www.r-bloggers.com/2015/12/how-to-learn-r-2/>
  * R jobs <https://www.r-users.com/> ► <#>
      o Submit a new job (it’s free) <https://www.r-users.com/submit-job/>
      o Browse latest jobs (also free) <https://www.r-users.com/>
  * Contact us <https://www.r-bloggers.com/contact-us/>

R-bloggers


R-bloggers


    R news and tutorials contributed by hundreds of R bloggers

<https://www.r-bloggers.com/>

  * Home <https://www.r-bloggers.com/>
  * About <https://www.r-bloggers.com/about/>
  * RSS <https://feeds.feedburner.com/RBloggers>
  * add your blog! <https://www.r-bloggers.com/add-your-blog/>
  * Learn R <https://www.r-bloggers.com/2015/12/how-to-learn-r-2/>
  * R jobs <https://www.r-users.com/>
      o Submit a new job (it’s free) <https://www.r-users.com/submit-job/>
      o Browse latest jobs (also free) <https://www.r-users.com/>
  * Contact us <https://www.r-bloggers.com/contact-us/>


  Sentiment analysis in R

Posted on May 16, 2021 by finnstats <https://www.r-bloggers.com/author/
finnstats/> in R bloggers <https://www.r-bloggers.com/category/r-
bloggers/> | 0 Comments

[This article was first published on *Methods – finnstats <https://
finnstats.com/index.php/2021/05/16/sentiment-analysis-in-r/>*, and
kindly contributed to R-bloggers <https://www.r-bloggers.com/>]. (You
can report issue about the content on this page here <https://www.r-
bloggers.com/contact-us/>)
------------------------------------------------------------------------
Want to share your content on R-bloggers?click here <https://www.r-
bloggers.com/add-your-blog/> if you have a blog, or here <http://r-
posts.com/> if you don't.

Share <https://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.r-
bloggers.com%2F2021%2F05%2Fsentiment-analysis-in-r-3%2F>Tweet <https://
twitter.com/intent/tweet?text=Sentiment%20analysis%20in%20R&url=https://
www.r-bloggers.com/2021/05/sentiment-analysis-in-r-3/&via=Rbloggers>

Sentiment analysis in R, In this article, we will discuss sentiment
analysis using R. We will make use of the syuzhet text package to
analyze the data and get scores for the corresponding words that are
present in the dataset.

The ultimate aim is to build a sentiment analysis model and identify the
words whether they are positive, negative, and also the magnitude of it.

In this article codes are mainly divided into loading data, build a
corpus, cleansing text, create term-document matrix, visualization, and
sentiment analysis.

Class imbalance in R <https://finnstats.com/index.php/2021/05/06/class-
imbalance/>


    Sentiment analysis in R

The following main packages are used in this article

  * tm for text mining operations like removing numbers, special
    characters, punctuations and stop words (Stop words in any language
    are the most commonly occurring words that have very little value
    for NLP and should be filtered out
  * word cloud for generating the word cloud plot.
  * syuzhet for sentiment scores and emotion classification
  * ggplot2 for plotting graphs


    What is Sentiment Analysis?

Sentiment Analysis is a process of extracting opinions that have
different scores like positive, negative or neutral.

Based on sentiment analysis, you can find out the nature of opinion or
sentences in text.

Sentiment Analysis is a type of classification where the data is
classified into different classes like positive or negative or happy,
sad, angry, etc.

Data Reshapes in R <https://finnstats.com/index.php/2020/11/26/data-
reshape-in-r/>


      Getting data

apple <- read.csv("D:/RStudio/SentimentAnalysis/Data1.csv", header = T)
str(apple)
apple <- read.csv("D:/RStudio/SentimentAnalysis/Data1.csv", header = T)
str(apple)

apple <- read.csv("D:/RStudio/SentimentAnalysis/Data1.csv", header = T)
str(apple)

This dataset contains 1000 observations and 16 variables but we are
interested only in one column that is ‘text’.

data.frame': 1000 obs. of  16 variables:
 $ text         : chr  "RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB" "RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB" "Let's see this break all timers. $AAPL 156.89" "RT @SylvaCap: Things might get ugly for $aapl with the iphone delay. With $aapl down that means almost all of t"| __truncated__ ... $ favorited    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ... $ favoriteCount: int  0 0 0 0 0 0 0 0 0 0 ...
 $ replyToSN    : chr  NA NA NA NA ... $ created      : chr  "2017-08-01 20:31:56" "2017-08-01 20:31:55" "2017-08-01 20:31:55" "2017-08-01 20:31:55" ...
 $ truncated    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ replyToSID   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ id           : num  8.92e+17 8.92e+17 8.92e+17 8.92e+17 8.92e+17 ...
 $ replyToUID   : num  NA NA NA NA NA NA NA NA NA NA ..
data.frame': 1000 obs. of  16 variables:
 $ text         : chr  "RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB" "RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB" "Let's see this break all timers. $AAPL 156.89" "RT @SylvaCap: Things might get ugly for $aapl with the iphone delay. With $aapl down that means almost all of t"| __truncated__ ... $ favorited    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ... $ favoriteCount: int  0 0 0 0 0 0 0 0 0 0 ...
 $ replyToSN    : chr  NA NA NA NA ... $ created      : chr  "2017-08-01 20:31:56" "2017-08-01 20:31:55" "2017-08-01 20:31:55" "2017-08-01 20:31:55" ...
 $ truncated    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ replyToSID   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ id           : num  8.92e+17 8.92e+17 8.92e+17 8.92e+17 8.92e+17 ...
 $ replyToUID   : num  NA NA NA NA NA NA NA NA NA NA ..

data.frame': 1000 obs. of  16 variables:
 $ text         : chr  "RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB" "RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB" "Let's see this break all timers. $AAPL 156.89" "RT @SylvaCap: Things might get ugly for $aapl with the iphone delay. With $aapl down that means almost all of t"| __truncated__ ... $ favorited    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ... $ favoriteCount: int  0 0 0 0 0 0 0 0 0 0 ...
 $ replyToSN    : chr  NA NA NA NA ... $ created      : chr  "2017-08-01 20:31:56" "2017-08-01 20:31:55" "2017-08-01 20:31:55" "2017-08-01 20:31:55" ...
 $ truncated    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ replyToSID   : num  NA NA NA NA NA NA NA NA NA NA ...
 $ id           : num  8.92e+17 8.92e+17 8.92e+17 8.92e+17 8.92e+17 ...
 $ replyToUID   : num  NA NA NA NA NA NA NA NA NA NA ..


      Build corpus

Once we loaded the dataset <https://github.com/finnstats/finnstats/blob/
main/Data1.csv> in R, the next step is to load that Vector or text data
as a Corpus. We can execute the same based on tm package in R.

Proportion test in R <https://finnstats.com/index.php/2020/09/16/
proportion-test-in-r/>

library(tm)
corpus <- iconv(apple$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
library(tm)
corpus <- iconv(apple$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])

library(tm)
corpus <- iconv(apple$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])

Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 5

[1] RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB
[2] RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB
[3] Let's see this break all timers. $AAPL 156.89
[4] RT @SylvaCap: Things might get ugly for $aapl with the iphone delay. With $aapl down that means almost all of the FANG stocks were down posâ€¦
[5] $AAPL - wow! This was supposed to be a throw-away quarter and AAPL beats by over 500 million in revenue! Trillion dollar company by 2018!
[1] RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB
[2] RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB
[3] Let's see this break all timers. $AAPL 156.89
[4] RT @SylvaCap: Things might get ugly for $aapl with the iphone delay. With $aapl down that means almost all of the FANG stocks were down posâ€¦
[5] $AAPL - wow! This was supposed to be a throw-away quarter and AAPL beats by over 500 million in revenue! Trillion dollar company by 2018!

[1] RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB
[2] RT @option_snipper: $AAPL beat on both eps and revenues. SEES 4Q REV. $49B-$52B, EST. $49.1B https://t.co/hfHXqj0IOB
[3] Let's see this break all timers. $AAPL 156.89
[4] RT @SylvaCap: Things might get ugly for $aapl with the iphone delay. With $aapl down that means almost all of the FANG stocks were down posâ€¦
[5] $AAPL - wow! This was supposed to be a throw-away quarter and AAPL beats by over 500 million in revenue! Trillion dollar company by 2018!


      Clean text

In the cleaning process first, we need to convert all the text into
lower case. Based on tm_map function can convert text into lower case.

corpus <- tm_map(corpus, tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus, tolower)
inspect(corpus[1:5])

corpus <- tm_map(corpus, tolower)
inspect(corpus[1:5])

Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 5

[1] rt @option_snipper: $aapl beat on both eps and revenues. sees 4q rev. $49b-$52b, est. $49.1b https://t.co/hfhxqj0iob
[2] rt @option_snipper: $aapl beat on both eps and revenues. sees 4q rev. $49b-$52b, est. $49.1b https://t.co/hfhxqj0iob
[3] let's see this break all timers. $aapl 156.89
[4] rt @sylvacap: things might get ugly for $aapl with the iphone delay. with $aapl down that means almost all of the fang stocks were down posâ€¦
[5] $aapl - wow! this was supposed to be a throw-away quarter and aapl beats by over 500 million in revenue! trillion dollar company by 2018!
[1] rt @option_snipper: $aapl beat on both eps and revenues. sees 4q rev. $49b-$52b, est. $49.1b https://t.co/hfhxqj0iob
[2] rt @option_snipper: $aapl beat on both eps and revenues. sees 4q rev. $49b-$52b, est. $49.1b https://t.co/hfhxqj0iob
[3] let's see this break all timers. $aapl 156.89
[4] rt @sylvacap: things might get ugly for $aapl with the iphone delay. with $aapl down that means almost all of the fang stocks were down posâ€¦
[5] $aapl - wow! this was supposed to be a throw-away quarter and aapl beats by over 500 million in revenue! trillion dollar company by 2018!

[1] rt @option_snipper: $aapl beat on both eps and revenues. sees 4q rev. $49b-$52b, est. $49.1b https://t.co/hfhxqj0iob
[2] rt @option_snipper: $aapl beat on both eps and revenues. sees 4q rev. $49b-$52b, est. $49.1b https://t.co/hfhxqj0iob
[3] let's see this break all timers. $aapl 156.89
[4] rt @sylvacap: things might get ugly for $aapl with the iphone delay. with $aapl down that means almost all of the fang stocks were down posâ€¦
[5] $aapl - wow! this was supposed to be a throw-away quarter and aapl beats by over 500 million in revenue! trillion dollar company by 2018!

Cleaning the text data one of the major parts is removing special
characters from the text. This is done using the tm_map() function to
replace all kinds of special characters.

One sample analysis in R <https://finnstats.com/index.php/2020/09/14/
one-sample-analysis-in-r/>

corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:5])

corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:5])

Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 5

[1] rt optionsnipper aapl beat on both eps and revenues sees 4q rev 49b52b est 491b httpstcohfhxqj0iob
[2] rt optionsnipper aapl beat on both eps and revenues sees 4q rev 49b52b est 491b httpstcohfhxqj0iob
[3] lets see this break all timers aapl 15689
[4] rt sylvacap things might get ugly for aapl with the iphone delay with aapl down that means almost all of the fang stocks were down posâ€¦
[5] aapl wow this was supposed to be a throwaway quarter and aapl beats by over 500 million in revenue trillion dollar company by 2018
[1] rt optionsnipper aapl beat on both eps and revenues sees 4q rev 49b52b est 491b httpstcohfhxqj0iob
[2] rt optionsnipper aapl beat on both eps and revenues sees 4q rev 49b52b est 491b httpstcohfhxqj0iob
[3] lets see this break all timers aapl 15689
[4] rt sylvacap things might get ugly for aapl with the iphone delay with aapl down that means almost all of the fang stocks were down posâ€¦
[5] aapl wow this was supposed to be a throwaway quarter and aapl beats by over 500 million in revenue trillion dollar company by 2018

[1] rt optionsnipper aapl beat on both eps and revenues sees 4q rev 49b52b est 491b httpstcohfhxqj0iob
[2] rt optionsnipper aapl beat on both eps and revenues sees 4q rev 49b52b est 491b httpstcohfhxqj0iob
[3] lets see this break all timers aapl 15689
[4] rt sylvacap things might get ugly for aapl with the iphone delay with aapl down that means almost all of the fang stocks were down posâ€¦
[5] aapl wow this was supposed to be a throwaway quarter and aapl beats by over 500 million in revenue trillion dollar company by 2018

In the text data numbers are commonly occur, we need to remove numbers
from the text data.

corpus <- tm_map(corpus, removeNumbers)
inspect(corpus[1:5])
corpus <- tm_map(corpus, removeNumbers)
inspect(corpus[1:5])

corpus <- tm_map(corpus, removeNumbers)
inspect(corpus[1:5])

Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 5

[1] rt optionsnipper aapl beat on both eps and revenues sees q rev bb est b httpstcohfhxqjiob
[2] rt optionsnipper aapl beat on both eps and revenues sees q rev bb est b httpstcohfhxqjiob
[3] lets see this break all timers aapl
[4] rt sylvacap things might get ugly for aapl with the iphone delay with aapl down that means almost all of the fang stocks were down posâ€¦
[5] aapl wow this was supposed to be a throwaway quarter and aapl beats by over million in revenue trillion dollar company by
[1] rt optionsnipper aapl beat on both eps and revenues sees q rev bb est b httpstcohfhxqjiob
[2] rt optionsnipper aapl beat on both eps and revenues sees q rev bb est b httpstcohfhxqjiob
[3] lets see this break all timers aapl
[4] rt sylvacap things might get ugly for aapl with the iphone delay with aapl down that means almost all of the fang stocks were down posâ€¦
[5] aapl wow this was supposed to be a throwaway quarter and aapl beats by over million in revenue trillion dollar company by

[1] rt optionsnipper aapl beat on both eps and revenues sees q rev bb est b httpstcohfhxqjiob
[2] rt optionsnipper aapl beat on both eps and revenues sees q rev bb est b httpstcohfhxqjiob
[3] lets see this break all timers aapl
[4] rt sylvacap things might get ugly for aapl with the iphone delay with aapl down that means almost all of the fang stocks were down posâ€¦
[5] aapl wow this was supposed to be a throwaway quarter and aapl beats by over million in revenue trillion dollar company by

Stop words are the most commonly occurring words in a language and have
very little value in terms of extracting useful information from the
text. Need to remove all stopwords from the text before the analysis.

Stop words mean like “the, is, at, on”.  stopwords in the tm_map()
function supports several languages like English, French, German,
Italian, and Spanish.

NULL Hypothesis <https://finnstats.com/index.php/2020/09/21/null-
hypothesis/>

cleanset <- tm_map(corpus, removeWords, stopwords('english'))
inspect(cleanset[1:5])
cleanset <- tm_map(corpus, removeWords, stopwords('english'))
inspect(cleanset[1:5])

cleanset <- tm_map(corpus, removeWords, stopwords('english'))
inspect(cleanset[1:5])

Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 5

[1] rt optionsnipper aapl beat eps revenues sees q rev bb est b httpstcohfhxqjiob
[2] rt optionsnipper aapl beat eps revenues sees q rev bb est b httpstcohfhxqjiob
[3] lets see break timers aapl
[4] rt sylvacap things might get ugly aapl iphone delay aapl means almost fang stocks posâ€¦
[5] aapl wow supposed throwaway quarter aapl beats million revenue trillion dollar company
[1] rt optionsnipper aapl beat eps revenues sees q rev bb est b httpstcohfhxqjiob
[2] rt optionsnipper aapl beat eps revenues sees q rev bb est b httpstcohfhxqjiob
[3] lets see break timers aapl
[4] rt sylvacap things might get ugly aapl iphone delay aapl means almost fang stocks posâ€¦
[5] aapl wow supposed throwaway quarter aapl beats million revenue trillion dollar company

[1] rt optionsnipper aapl beat eps revenues sees q rev bb est b httpstcohfhxqjiob
[2] rt optionsnipper aapl beat eps revenues sees q rev bb est b httpstcohfhxqjiob
[3] lets see break timers aapl
[4] rt sylvacap things might get ugly aapl iphone delay aapl means almost fang stocks posâ€¦
[5] aapl wow supposed throwaway quarter aapl beats million revenue trillion dollar company

Depends on your dataset if links contain the dataset remove the same.

removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])

removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])

Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 5

[1] rt optionsnipper aapl beat eps revenues sees q rev bb est b
[2] rt optionsnipper aapl beat eps revenues sees q rev bb est b
[3] lets see break timers aapl
[4] rt sylvacap things might get ugly aapl iphone delay aapl means almost fang stocks posâ€¦
[5] aapl wow supposed throwaway quarter aapl beats million revenue trillion dollar company
[1] rt optionsnipper aapl beat eps revenues sees q rev bb est b
[2] rt optionsnipper aapl beat eps revenues sees q rev bb est b
[3] lets see break timers aapl
[4] rt sylvacap things might get ugly aapl iphone delay aapl means almost fang stocks posâ€¦
[5] aapl wow supposed throwaway quarter aapl beats million revenue trillion dollar company

[1] rt optionsnipper aapl beat eps revenues sees q rev bb est b
[2] rt optionsnipper aapl beat eps revenues sees q rev bb est b
[3] lets see break timers aapl
[4] rt sylvacap things might get ugly aapl iphone delay aapl means almost fang stocks posâ€¦
[5] aapl wow supposed throwaway quarter aapl beats million revenue trillion dollar company

Text stemming – which reduces words to their root form

cleanset <- tm_map(cleanset, removeWords, c('aapl', 'apple'))
cleanset <- tm_map(cleanset, gsub,
                   pattern = 'stocks',
                   replacement = 'stock')
cleanset <- tm_map(cleanset, stemDocument)
cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset, removeWords, c('aapl', 'apple'))
cleanset <- tm_map(cleanset, gsub,
                   pattern = 'stocks',
                   replacement = 'stock')
cleanset <- tm_map(cleanset, stemDocument)
cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])

cleanset <- tm_map(cleanset, removeWords, c('aapl', 'apple'))
cleanset <- tm_map(cleanset, gsub,
                   pattern = 'stocks',
                   replacement = 'stock')
cleanset <- tm_map(cleanset, stemDocument)
cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])

Metadata: corpus specific: 1, document level (indexed): 0
Content: documents: 5

[1] rt optionsnipp beat ep revenu see q rev bb est b
[2] rt optionsnipp beat ep revenu see q rev bb est b
[3] let see break timer
[4] rt sylvacap thing might get ugli iphon delay mean almost fang stock posâ€¦
[5] wow suppos throwaway quarter beat million revenu trillion dollar compani
[1] rt optionsnipp beat ep revenu see q rev bb est b
[2] rt optionsnipp beat ep revenu see q rev bb est b
[3] let see break timer
[4] rt sylvacap thing might get ugli iphon delay mean almost fang stock posâ€¦
[5] wow suppos throwaway quarter beat million revenu trillion dollar compani

[1] rt optionsnipp beat ep revenu see q rev bb est b
[2] rt optionsnipp beat ep revenu see q rev bb est b
[3] let see break timer
[4] rt sylvacap thing might get ugli iphon delay mean almost fang stock posâ€¦
[5] wow suppos throwaway quarter beat million revenu trillion dollar compani


      Term document matrix

After cleansing the textual content data, the following step is to
matter the incidence of every word, to perceive famous or trending topics.

Discriminant Analysis in R <https://finnstats.com/index.php/2020/09/24/
discriminant-analysis-in-r/>

Using the function TermDocumentMatrix() from the textual content mining
package, you may construct a Document Matrix – a table containing the
frequency of words.

tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
Docs
 Terms         1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
   beat        1 1 0 0 1 0 0 0 0  0  0  0  0  0  0  0  0  0  1  0
   est         1 1 0 0 0 2 2 0 0  0  0  0  0  2  0  0  2  2  0  0
   optionsnipp 1 1 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   rev         1 1 0 0 0 1 1 0 0  1  0  0  1  0  0  0  1  1  0  0
   revenu      1 1 0 0 1 0 0 0 1  0  0  0  0  0  0  0  0  0  0  0
   see         1 1 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   break       0 0 1 0 0 0 0 1 1  1  1  0  0  0  0  0  0  0  0  0
   let         0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   timer       0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   almost      0 0 0 1 0 0 0 0 0  0  0  0  0  0  1  0  0  0  0  0
tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
Docs
 Terms         1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
   beat        1 1 0 0 1 0 0 0 0  0  0  0  0  0  0  0  0  0  1  0
   est         1 1 0 0 0 2 2 0 0  0  0  0  0  2  0  0  2  2  0  0
   optionsnipp 1 1 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   rev         1 1 0 0 0 1 1 0 0  1  0  0  1  0  0  0  1  1  0  0
   revenu      1 1 0 0 1 0 0 0 1  0  0  0  0  0  0  0  0  0  0  0
   see         1 1 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   break       0 0 1 0 0 0 0 1 1  1  1  0  0  0  0  0  0  0  0  0
   let         0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   timer       0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   almost      0 0 0 1 0 0 0 0 0  0  0  0  0  0  1  0  0  0  0  0

tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
Docs
 Terms         1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
   beat        1 1 0 0 1 0 0 0 0  0  0  0  0  0  0  0  0  0  1  0
   est         1 1 0 0 0 2 2 0 0  0  0  0  0  2  0  0  2  2  0  0
   optionsnipp 1 1 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   rev         1 1 0 0 0 1 1 0 0  1  0  0  1  0  0  0  1  1  0  0
   revenu      1 1 0 0 1 0 0 0 1  0  0  0  0  0  0  0  0  0  0  0
   see         1 1 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   break       0 0 1 0 0 0 0 1 1  1  1  0  0  0  0  0  0  0  0  0
   let         0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   timer       0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
   almost      0 0 0 1 0 0 0 0 0  0  0  0  0  0  1  0  0  0  0  0


      Bar plot

Plotting the words using a bar chart is a good basic way to visualize
this word’s frequent data. Simply you can create a bar chart for
visualization.

w <- rowSums(tdm)
w <- subset(w, w>=25)
barplot(w,
        las = 2,
        col = rainbow(50))
w <- rowSums(tdm)
w <- subset(w, w>=25)
barplot(w,
        las = 2,
        col = rainbow(50))

w <- rowSums(tdm)
w <- subset(w, w>=25)
barplot(w,
        las = 2,
        col = rainbow(50))


      Word cloud

A word cloud is one of the most popular ways to visualize and analyze
text data. It’s an image composed of keywords found within a body of
text, where the size of each word indicates its count in that body of text.

Latest data science job vacancies <https://finnstats.com/>

Use the word frequency data frame (table) created previously to generate
the word cloud.

library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
          freq = w,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5, 0.3),
          rot.per = 0.7)
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
          freq = w,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5, 0.3),
          rot.per = 0.7)

library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
          freq = w,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5, 0.3),
          rot.per = 0.7)

library(wordcloud2)
w <- data.frame(names(w), w)
colnames(w) <- c('word', 'freq')
wordcloud2(w,
           size = 0.7,
           shape = 'triangle',
           rotateRatio = 0.5,
           minSize = 1)
library(wordcloud2)
w <- data.frame(names(w), w)
colnames(w) <- c('word', 'freq')
wordcloud2(w,
           size = 0.7,
           shape = 'triangle',
           rotateRatio = 0.5,
           minSize = 1)

library(wordcloud2)
w <- data.frame(names(w), w)
colnames(w) <- c('word', 'freq')
wordcloud2(w,
           size = 0.7,
           shape = 'triangle',
           rotateRatio = 0.5,
           minSize = 1)


      Sentiment analysis

We discussed earlier sentiments can be classified as positive, neutral,
or negative. They can also be represented on a numeric scale, to better
express the degree of positive or negative strength of the sentiment
contained in a body of text.

This example uses the Syuzhet package for generating sentiment scores,
which has four sentiment dictionaries and offers a method for accessing
the sentiment extraction tool developed in the NLP group at Stanford.

Decision Trees in R <https://finnstats.com/index.php/2021/04/19/
decision-trees-in-r/>

The get_sentiment function accepts two arguments: a character vector (of
sentences or words) and a method. The selected method determines which
of the four available sentiment extraction methods will be used. The
four methods are syuzhet (this is the default), bing, afinn and nrc.

Each method uses a different scale and hence returns slightly different
results.

library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)

library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)


      Getting Data

apple <- read.csv("D:/RStudio/SentimentAnalysis/Data1.csv", header = T)
tweets <- iconv(apple$text)
apple <- read.csv("D:/RStudio/SentimentAnalysis/Data1.csv", header = T)
tweets <- iconv(apple$text)

apple <- read.csv("D:/RStudio/SentimentAnalysis/Data1.csv", header = T)
tweets <- iconv(apple$text)


        Obtain sentiment scores

s <- get_nrc_sentiment(tweets)
head(s)
anger anticipation disgust fear joy sadness surprise trust negative positive
1     0            0       0    0   0       0        0     0        0        1
2     0            0       0    0   0       0        0     0        0        1
3     0            0       0    0   0       0        1     0        0        0
4     1            0       2    2   0       1        0     0        3        0
5     0            0       0    0   0       0        0     0        0        0
6     0            0       0    0   0       0        0     0        0        0
s <- get_nrc_sentiment(tweets)
head(s)
anger anticipation disgust fear joy sadness surprise trust negative positive
1     0            0       0    0   0       0        0     0        0        1
2     0            0       0    0   0       0        0     0        0        1
3     0            0       0    0   0       0        1     0        0        0
4     1            0       2    2   0       1        0     0        3        0
5     0            0       0    0   0       0        0     0        0        0
6     0            0       0    0   0       0        0     0        0        0

s <- get_nrc_sentiment(tweets)
head(s)
anger anticipation disgust fear joy sadness surprise trust negative positive
1     0            0       0    0   0       0        0     0        0        1
2     0            0       0    0   0       0        0     0        0        1
3     0            0       0    0   0       0        1     0        0        0
4     1            0       2    2   0       1        0     0        3        0
5     0            0       0    0   0       0        0     0        0        0
6     0            0       0    0   0       0        0     0        0        0

Its ranging from anger to trust, Negative and Positive.

get_nrc_sentiment(ugly)
anger anticipation disgust fear joy sadness surprise trust negative positive
 0            0       1    0   0       0        0     0        1        0
get_nrc_sentiment(ugly)
anger anticipation disgust fear joy sadness surprise trust negative positive
 0            0       1    0   0       0        0     0        1        0

get_nrc_sentiment(ugly)
anger anticipation disgust fear joy sadness surprise trust negative positive
 0            0       1    0   0       0        0     0        1        0

Ugly has scores on disgust and negative.


      Bar plot

barplot(colSums(s),
        las = 2,
        col = rainbow(10),
        ylab = 'Count',
        main = 'Sentiment Scores Tweets')
barplot(colSums(s),
        las = 2,
        col = rainbow(10),
        ylab = 'Count',
        main = 'Sentiment Scores Tweets')

barplot(colSums(s),
        las = 2,
        col = rainbow(10),
        ylab = 'Count',
        main = 'Sentiment Scores Tweets')

Sentiment scores more on negative followed by anticipation and positive,
trust and fear.


    Conclusion

This article explained reading text data into R, corpus creation, data
cleaning, transformations and explained how to create a word frequency
and word clouds to identify the occurrence of the text.

Identification of sentiment scores, which proved useful in assigning a
numeric value to strength (of positivity or negativity) of sentiments in
the text and allowed interpreting score of the text.

Principal Component Analysis in R <https://finnstats.com/
index.php/2021/05/07/pca/>

<https://www.addtoany.com/add_to/whatsapp?
linkurl=https%3A%2F%2Ffinnstats.com%2Findex.php%2F2021%2F05%2F16%2Fsentiment-analysis-in-r%2F&linkname=Sentiment%20analysis%20in%20R><https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Ffinnstats.com%2Findex.php%2F2021%2F05%2F16%2Fsentiment-analysis-in-r%2F&linkname=Sentiment%20analysis%20in%20R><https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Ffinnstats.com%2Findex.php%2F2021%2F05%2F16%2Fsentiment-analysis-in-r%2F&linkname=Sentiment%20analysis%20in%20R><https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Ffinnstats.com%2Findex.php%2F2021%2F05%2F16%2Fsentiment-analysis-in-r%2F&linkname=Sentiment%20analysis%20in%20R><https://www.addtoany.com/add_to/telegram?linkurl=https%3A%2F%2Ffinnstats.com%2Findex.php%2F2021%2F05%2F16%2Fsentiment-analysis-in-r%2F&linkname=Sentiment%20analysis%20in%20R><https://www.addtoany.com/share#url=https%3A%2F%2Ffinnstats.com%2Findex.php%2F2021%2F05%2F16%2Fsentiment-analysis-in-r%2F&%23038;title=Sentiment%20analysis%20in%20R>

The post Sentiment analysis in R <https://finnstats.com/
index.php/2021/05/16/sentiment-analysis-in-r/> appeared first on
finnstats <https://finnstats.com/>.


      /Related/

Share <https://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.r-
bloggers.com%2F2021%2F05%2Fsentiment-analysis-in-r-3%2F>Tweet <https://
twitter.com/intent/tweet?text=Sentiment%20analysis%20in%20R&url=https://
www.r-bloggers.com/2021/05/sentiment-analysis-in-r-3/&via=Rbloggers>

To *leave a comment* for the author, please follow the link and comment
on their blog: *Methods – finnstats <https://finnstats.com/
index.php/2021/05/16/sentiment-analysis-in-r/>*.
------------------------------------------------------------------------
R-bloggers.com <https://www.r-bloggers.com/> offers *daily e-mail
updates <https://feedburner.google.com/fb/a/mailverify?uri=RBloggers>*
about R <https://www.r-project.org/> news and tutorials about learning R
<https://www.r-bloggers.com/how-to-learn-r-2/> and many other topics.
Click here if you're looking to post or find an R/data-science job
<https://www.r-users.com/>.
------------------------------------------------------------------------
Want to share your content on R-bloggers?click here <https://www.r-
bloggers.com/add-your-blog/> if you have a blog, or here <http://r-
posts.com/> if you don't.

← Previous post <https://www.r-bloggers.com/2021/05/time-series-cross-
validation-using-fable/>
Next post → <https://www.r-bloggers.com/2021/05/unbalanced-sampling/>

<https://feeds.feedburner.com/RBloggers>


        Most viewed posts (weekly)

  * Which data science skills are important ($50,000 increase in salary
    in 6-months) <https://www.r-bloggers.com/2022/03/which-data-science-
    skills-are-important-50000-increase-in-salary-in-6-months/>
  * PCA vs Autoencoders for Dimensionality Reduction <https://www.r-
    bloggers.com/2018/07/pca-vs-autoencoders-for-dimensionality-reduction/>
  * Better Sentiment Analysis with sentiment.ai <https://www.r-
    bloggers.com/2022/03/better-sentiment-analysis-with-sentiment-ai/>
  * Self-documenting plots in ggplot2 <https://www.r-
    bloggers.com/2022/03/self-documenting-plots-in-ggplot2/>
  * 5 Ways to Subset a Data Frame in R <https://www.r-
    bloggers.com/2016/11/5-ways-to-subset-a-data-frame-in-r/>
  * How to write the first for loop in R <https://www.r-
    bloggers.com/2015/12/how-to-write-the-first-for-loop-in-r/>
  * Dashboards in R Shiny <https://www.r-bloggers.com/2022/03/
    dashboards-in-r-shiny/>


        Sponsors

conjoint ly <https://conjointly.com/solutions/insights-explorer/?
utm_source=rbloggers>
------------------------------------------------------------------------
------------------------------------------------------------------------
jumping rivers <https://www.jumpingrivers.com/posit/?
utm_source=rbloggers&utm_campaign=paid&utm_medium=banner>
------------------------------------------------------------------------
Six Sigma Online Training <https://www.6sigma.us/six-sigma-online-
training.php>
------------------------------------------------------------------------
Our ads respect your privacy. Read our Privacy Policy page <https://
www.r-bloggers.com/privacy-policy/> to learn more.
------------------------------------------------------------------------

*Contact us <https://www.r-bloggers.com/contact-us/>* if you wish to
help support R-bloggers, and place *your banner here*.


        Recent Posts

  * Deutschsprachiges Online Shiny Training von eoda <https://www.r-
    bloggers.com/2022/03/deutschsprachiges-online-shiny-training-von-eoda/>
  * How to Calculate a Bootstrap Standard Error in R <https://www.r-
    bloggers.com/2022/03/how-to-calculate-a-bootstrap-standard-error-in-r/>
  * Dashboards in R Shiny <https://www.r-bloggers.com/2022/03/
    dashboards-in-r-shiny/>
  * Some R Conferences for 2022 <https://www.r-bloggers.com/2022/03/
    some-r-conferences-for-2022/>
  * Curating Your Data Science Content on RStudio Connect <https://
    www.r-bloggers.com/2022/03/curating-your-data-science-content-on-
    rstudio-connect/>
  * Adding competing risks in survival data generation <https://www.r-
    bloggers.com/2022/03/adding-competing-risks-in-survival-data-
    generation/>
  * measurement units <https://www.r-bloggers.com/2022/03/measurement-
    units/>
  * Confidence Intervals Explained <https://www.r-bloggers.com/2022/03/
    confidence-intervals-explained/>
  * The E8 root polytope <https://www.r-bloggers.com/2022/03/the-e8-
    root-polytope/>
  * extinction minus one <https://www.r-bloggers.com/2022/03/extinction-
    minus-one/>
  * A zsh Helper Script For Updating macOS RStudio Daily Electron +
    Quarto CLI Installs <https://www.r-bloggers.com/2022/03/a-zsh-
    helper-script-for-updating-macos-rstudio-daily-electron-quarto-cli-
    installs/>
  * Predictive Analytics Models in R <https://www.r-
    bloggers.com/2022/03/predictive-analytics-models-in-r/>
  * repoRter.nih: a convenient R interface to the NIH RePORTER Project
    API <https://www.r-bloggers.com/2022/03/reporter-nih-a-convenient-r-
    interface-to-the-nih-reporter-project-api/>
  * Markov Chain Introduction in R <https://www.r-bloggers.com/2022/03/
    markov-chain-introduction-in-r/>
  * Dual axis charts – how to make them and why they can be useful
    <https://www.r-bloggers.com/2022/03/dual-axis-charts-how-to-make-
    them-and-why-they-can-be-useful/>


        RSS <https://feeds.feedburner.com/Rjobs> Jobs for R-users
        <https://www.r-users.com/>

  * Junior Data Scientist / Quantitative economist <https://www.r-
    users.com/jobs/junior-data-scientist-quantitative-economist/>
  * Senior Quantitative Analyst <https://www.r-users.com/jobs/senior-
    quantitative-analyst/>
  * R programmer <https://www.r-users.com/jobs/r-programmer-4/>
  * Data Scientist – CGIAR Excellence in Agronomy (Ref No: DDG-R4D/DS/1/
    CG/EA/06/20) <https://www.r-users.com/jobs/data-scientist-cgiar-
    excellence-in-agronomy-ref-no-ddg-r4d-ds-1-cg-ea-06-20/>
  * Data Analytics Auditor, Future of Audit Lead @ London or Newcastle
    <https://www.r-users.com/jobs/data-analytics-auditor-future-of-
    audit-lead-london-or-newcastle/>


        RSS <https://feeds.feedburner.com/Python-bloggers> python-
        bloggers.com (python/data-science news) <https://python-
        bloggers.com/>

  * Explaining a Keras _neural_ network predictions with the-teller
    <https://python-bloggers.com/2022/03/explaining-a-keras-_neural_-
    network-predictions-with-the-teller/>
  * Object Oriented Programming in Python – What and Why? <https://
    python-bloggers.com/2022/03/object-oriented-programming-in-python-
    what-and-why/>
  * Dunn Index for K-Means Clustering Evaluation <https://python-
    bloggers.com/2022/03/dunn-index-for-k-means-clustering-evaluation/>
  * Installing Python and Tensorflow with Jupyter Notebook
    Configurations <https://python-bloggers.com/2022/03/installing-
    python-and-tensorflow-with-jupyter-notebook-configurations/>
  * How to Get Twitter Data using Python <https://python-
    bloggers.com/2022/03/how-to-get-twitter-data-using-python/>
  * Visualizations with Altair <https://python-bloggers.com/2022/02/
    visualizations-with-altair/>
  * Spelling Corrector Program in Python <https://python-
    bloggers.com/2022/02/spelling-corrector-program-in-python/>

*Full list of contributing R-bloggers <https://www.r-bloggers.com/blogs-
list/>*


        Archives

Archives


        Other sites

  * Jobs for R-users <https://www.r-users.com/>
  * SAS blogs <http://www.proc-x.com/>

Copyright © 2022 | MH Corporate basic by MH Themes <https://
www.mhthemes.com/>


  /Never miss an update! /
  *Subscribe to R-bloggers* to receive
  e-mails with the latest R posts.
  (You will not see this message again.)

Submit

Click here to close (This popup will not appear again) <#>
